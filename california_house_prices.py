# -*- coding: utf-8 -*-
"""California House Prices.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k4j-AqZ5I3e6DJRID_7NMjdq_65BGEIK
"""

# Commented out IPython magic to ensure Python compatibility.
#Initialization:
# %reload_ext autoreload
# %autoreload 2
# %matplotlib inline

#Downloading the Libraries and Dependencies:
import sys, os, tarfile, urllib.request
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.model_selection import StratifiedShuffleSplit
from pandas.plotting import scatter_matrix
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV

#Visualization Dependencies:
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc("axes", labelsize=14)
mpl.rc("xtick", labelsize=12)
mpl.rc("ytick", labelsize=12)

PROJECT_ROOT_DIR = "."
ID = "California Housing"
IMAGE_PATH = os.path.join(PROJECT_ROOT_DIR, "Images", ID)
if not os.path.isdir(IMAGE_PATH):
    os.makedirs(IMAGE_PATH)
def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
  path = os.path.join(IMAGE_PATH, fig_id + "." + fig_extension)
  print("Saving Figure", fig_id)
  if tight_layout:
    plt.tight_layout()
  plt.savefig(path, format=fig_extension, dpi=resolution)

#Ignoring the Warnings:
import warnings
warnings.filterwarnings(action="ignore", message="^ internal")

#Downloading the Dependencies:
# !pip install sweetviz
# import sweetviz as sv
import IPython

#Getting the Data:
import urllib.request
DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml2/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
  if not os.path.isdir(housing_path):
    os.makedirs(housing_path)
  tgz_path = os.path.join(housing_path, "housing.tgz")
  urllib.request.urlretrieve(housing_url, tgz_path)
  housing_tgz = tarfile.open(tgz_path)
  housing_tgz.extractall(path=housing_path)
  housing_tgz.close()

fetch_housing_data()                                                           # Initializing the Function.

#Getting the Data:
def load_housing_data():
    HOUSING_PATH = os.path.join("datasets", "housing")
    csv_path = os.path.join(HOUSING_PATH, "housing.csv")
    return pd.read_csv(csv_path)

#Inspecting the Data:
housing = load_housing_data()
housing.head(7)

#title housing_median_age vs total_rooms

from matplotlib import pyplot as plt
housing.plot(kind='scatter', x='housing_median_age', y='total_rooms', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

#@ Business Understanding:
print("Business Understanding:")
print("The goal of this project is to predict the median housing prices in California using machine learning techniques.")
print("Understanding the factors influencing housing prices can help stakeholders such as real estate investors, developers, and home buyers to make informed decisions.")
print("We will analyze the California housing dataset to build a predictive model that accurately estimates housing prices based on various features.")

#Exploratory Data Analysis (EDA):
print("\nExploratory Data Analysis:")
# Summary statistics
print("Summary Statistics:")
print(housing.describe())

# Histograms of numerical attributes
housing.hist(bins=50, figsize=(20,15))
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Select only numeric columns
housing_numeric = housing.select_dtypes(include=['number'])

# Correlation matrix for numeric columns
corr_matrix = housing_numeric.corr()

# Plotting the correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title("Correlation Matrix")
plt.show()

# Scatter plot of median_house_value vs other numerical attributes
attributes = ["median_house_value", "median_income", "total_rooms", "housing_median_age"]
scatter_matrix(housing[attributes], figsize=(12, 8))
plt.show()

#Data Preparation:
print("\nData Preparation:")

#Separate features and target variable
X = housing.drop("median_house_value", axis=1)
y = housing["median_house_value"]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Numerical and categorical pipelines
num_attribs = list(X.select_dtypes(include=[np.number]).columns)
cat_attribs = ["ocean_proximity"]

num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy="median")),
    ('std_scaler', StandardScaler()),
])

cat_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy="most_frequent")),
    ('one_hot_encoder', OneHotEncoder()),
])

# Full pipeline handling both numerical and categorical features
full_pipeline = ColumnTransformer([
    ("num", num_pipeline, num_attribs),
    ("cat", cat_pipeline, cat_attribs),
])

print(housing.columns)

# Load the data
housing = load_housing_data()

# Split data into features (X) and target (y)
X = housing.drop("median_house_value", axis=1)
y = housing["median_house_value"]

# Define numeric and categorical features
numeric_features = X.select_dtypes(include=[np.number]).columns
categorical_features = ["ocean_proximity"]  # Assuming 'ocean_proximity' is categorical

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Preprocessing pipeline for numeric features
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Preprocessing pipeline for categorical features
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Fit and transform training data
X_train_prepared = preprocessor.fit_transform(X_train)

# Transform testing data (only transform, no fit)
X_test_prepared = preprocessor.transform(X_test)

# Import libraries for modeling
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Initialize RandomForestRegressor model
forest_reg = RandomForestRegressor(random_state=42)

# Fit model to the training data
forest_reg.fit(X_train_prepared, y_train)

# Predictions on training set
y_train_pred = forest_reg.predict(X_train_prepared)

# Calculate RMSE on training set
rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))
print(f"RMSE on training set: {rmse_train}")

# Predictions on test set
y_test_pred = forest_reg.predict(X_test_prepared)

# Calculate RMSE on test set
rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))
print(f"RMSE on test set: {rmse_test}")

# Evaluate model using additional metrics
from sklearn.metrics import r2_score

# Calculate R2 score on training set
r2_train = r2_score(y_train, y_train_pred)
print(f"R2 score on training set: {r2_train}")

# Calculate R2 score on test set
r2_test = r2_score(y_test, y_test_pred)
print(f"R2 score on test set: {r2_test}")

# Plot predictions vs actual values for test set
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.xlabel('Actual Median House Value')
plt.ylabel('Predicted Median House Value')
plt.title('Actual vs Predicted Median House Value (Test Set)')
plt.grid(True)
plt.show()

#@ Hyperparameter Tuning:
print("\nHyperparameter Tuning:")
# Inisiasi model Random Forest Regressor
forest_reg = RandomForestRegressor(random_state=42)

# Definisi grid hyperparameter untuk RandomizedSearchCV
param_distribs = {
    'n_estimators': [100, 200, 300],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Inisialisasi RandomizedSearchCV
rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,
                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)

# Persiapkan data untuk training
X = housing.drop("median_house_value", axis=1)
y = housing["median_house_value"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Skalasi fitur numerik
num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy="median")),
    ('std_scaler', StandardScaler()),
])

# Transformasi data numerik
num_attribs = list(X.select_dtypes(include=[np.number]).columns)
cat_attribs = ["ocean_proximity"]

# Gabungkan pipeline numerik dan categorical
full_pipeline = ColumnTransformer([
    ("num", num_pipeline, num_attribs),
    ("cat", OneHotEncoder(), cat_attribs),
])

#Data Understanding:
print(f"Dataset contains {housing.shape[0]} rows and {housing.shape[1]} columns.")
print("Columns in the dataset:")
print(housing.columns)

# Check for missing values
missing_values = housing.isnull().sum()
print("Missing values in each column:")
print(missing_values[missing_values > 0])

#@ Evaluation:
def evaluate_model(y_test, y_pred):
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    return mae, rmse, r2

print("Linear Regression Evaluation:")
mae_lr, rmse_lr, r2_lr = evaluate_model(y_test, y_pred_lr)
print(f"MAE: {mae_lr}, RMSE: {rmse_lr}, R2: {r2_lr}")

#@ Visualization:
# Visualize housing_median_age vs total_rooms
housing.plot(kind='scatter', x='housing_median_age', y='total_rooms', s=32, alpha=.8)
plt.gca().spines[['top', 'right']].set_visible(False)
plt.title("housing_median_age vs total_rooms")
plt.xlabel("housing_median_age")
plt.ylabel("total_rooms")
plt.show()